{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fd00c1",
   "metadata": {},
   "source": [
    "**Knee Detection Model for Lateral Knee Joint (Main Jupyter Notebook)**\n",
    "\n",
    "\n",
    "Version 1.1\n",
    "\n",
    "By Lu Yik Ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634f760",
   "metadata": {},
   "source": [
    "**Step 1: Program Requirements**\n",
    "\n",
    "This program requires preinstalled modules. This step provides a guide on the required modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511a627",
   "metadata": {},
   "source": [
    "**Step 1.3**      \n",
    "\n",
    "Confirm Module Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules.\n",
    "\n",
    "# Run this code box once to ensure all modules are installed.\n",
    "\n",
    "import torch\n",
    "\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "\n",
    "import pydicom\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import shutil\n",
    "import ultralytics\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "\n",
    "print(\"All modules imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dd300",
   "metadata": {},
   "source": [
    "**Step 2: Preparing Images for LabelMe**\n",
    "\n",
    "These steps mainly consist of processing the images retrieved from DICOM files. These steps aid in labelling bounding boxes with LabelMe, and in the training the YOLO model.\n",
    "\n",
    "You should run this step for both preparing training and inference images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23989ed0",
   "metadata": {},
   "source": [
    "**Step 2.1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ccddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original file in dataset is DICOM, we need to convert to PNG so that we can check the each step output easily\n",
    "\n",
    "from KneeDetectionLateral_ImageProcessing import dicom_to_png\n",
    "\n",
    "# Input path to DICOM Files\n",
    "dicom_files_lateralL = glob.glob(\"./DICOM Files/**/**/**/**/LL**/**\")\n",
    "dicom_files_lateralR = glob.glob(\"./DICOM Files/**/**/**/**/RL**/**\")\n",
    "\n",
    "count = 0 \n",
    "\n",
    "# Convert all DICOM files to PNG\n",
    "for file in dicom_files_lateralL:\n",
    "    if not file.endswith(\".png\") and not file.endswith(\".json\") and not file.endswith(\".txt\"):\n",
    "        dicom_to_png(file)\n",
    "        count = count + 1\n",
    "\n",
    "print(\"LL Images: {}\".format(count))\n",
    "count = 0 \n",
    "\n",
    "# Convert all DICOM files to PNG\n",
    "for file in dicom_files_lateralR:\n",
    "    if not file.endswith(\".png\") and not file.endswith(\".json\") and not file.endswith(\".txt\"):\n",
    "        dicom_to_png(file)\n",
    "        count = count + 1\n",
    "\n",
    "print(\"RL Images: {}\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b113ae5",
   "metadata": {},
   "source": [
    "**Step 2.2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac94f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip all right lateral images to left lateral images\n",
    "\n",
    "from KneeDetectionLateral_ImageProcessing import flip_png\n",
    "\n",
    "# Input target PNG files\n",
    "png_files = glob.glob(\"./DICOM Files/**/**/**/**/RL**/*.png\")\n",
    "\n",
    "# Flip all PNG files\n",
    "for file in png_files:\n",
    "    \n",
    "    # Prevents flipping twice\n",
    "    if not file.endswith(\"_C.png\") and not file.endswith(\"_F.png\") and not file.endswith(\"_N.png\"):\n",
    "      flip_png(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e5ff5",
   "metadata": {},
   "source": [
    "**Step 2.3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a45149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code should be used specifically for the lateral images to remove the second joint.\n",
    "\n",
    "from KneeDetectionLateral_ImageProcessing import split_png_lateral\n",
    "\n",
    "# Input target PNG files\n",
    "png_files = glob.glob(\"./DICOM Files/**/**/**/**/**/*.png\")\n",
    "\n",
    "# Adjust percentage of pixels to be cropped here:\n",
    "crop_value = 0.3\n",
    "\n",
    "# Adjust which side of the image should be cropped\n",
    "crop_side = \"Left\"\n",
    "\n",
    "# Crop all PNG files\n",
    "for file in png_files:\n",
    "    if not file.endswith(\"_C.png\") and not file.endswith(\"_N.png\"):\n",
    "        split_png_lateral(file, crop_value, crop_side)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f616fc7",
   "metadata": {},
   "source": [
    "**Step 2.4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f539232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise images \n",
    "\n",
    "from KneeDetectionLateral_ImageProcessing import normal_img\n",
    "\n",
    "# Input target PNG files\n",
    "input_files_L = glob.glob(\"./DICOM Files/**/**/**/**/LL**/*_C.png\")\n",
    "\n",
    "input_files_R = glob.glob(\"./DICOM Files/**/**/**/**/RL**/*_F_C.png\")\n",
    "\n",
    "# Input output directory\n",
    "output_dir = \"./Images for Labeling\"\n",
    "\n",
    "for file in input_files_L:\n",
    "    if file.endswith(\"_C.png\"):\n",
    "      normal_img(file, True, output_dir)\n",
    "\n",
    "for file in input_files_R:\n",
    "    if file.endswith(\"_F_C.png\"):\n",
    "      normal_img(file, True, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e2093",
   "metadata": {},
   "source": [
    "**Step 3: Creating Bounding Boxes with LabelMe**\n",
    "\n",
    "LabelMe is the main program used to create the bounding box of the region of interest. These steps provide a guide to the specific method to use LabelMe for this program.\n",
    "\n",
    "This Jupyter Notebook is not required for these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4a5f2",
   "metadata": {},
   "source": [
    "**Step 4: Creating YOLO Required Dataset**\n",
    "\n",
    "These steps convert the current images and labels into a dataset required for training a YOLO model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818060e",
   "metadata": {},
   "source": [
    "**Step 4.2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b540848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting LabelMe bounding boxes to YOLO bounding boxes\n",
    "# LabelMe: (x-min, y-min, x-max, y-max) in .json file\n",
    "# YOLO: (x1, y1, x2, y2...) in .txt file\n",
    "# LabelMe and YOLO shares same coordinate system, but YOLO requires normalisation of coordinates\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import LMtoYOLO\n",
    "\n",
    "# Input target JSON files\n",
    "json_files = glob.glob(\"./Training Images/**\")\n",
    "\n",
    "# Input output directory\n",
    "output_dir = \"./Training Labels\"\n",
    "\n",
    "# Run conversion for all files\n",
    "for file in json_files:\n",
    "    if file.endswith(\".json\"):\n",
    "        LMtoYOLO(file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a35eb1",
   "metadata": {},
   "source": [
    "**Step 4.3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the current data from directories into training and testing data, and saves data to YOLO required directory format\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import YOLOTTSplit\n",
    "\n",
    "# Input and Output Directories\n",
    "image_dir = \"./Training Images\"\n",
    "txt_dir = \"./Training Labels\"\n",
    "target_dir = \"./YOLO Dataset\"\n",
    "\n",
    "# Percentage of data to be used at testing\n",
    "test_section = 0.20\n",
    "\n",
    "YOLOTTSplit(image_dir, txt_dir, target_dir, test_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf36c0",
   "metadata": {},
   "source": [
    "**Step 4.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the required .yaml file required for YOLO\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import createYaml\n",
    "\n",
    "# Path to YOLO dataset\n",
    "path = \"./YOLO Dataset\"\n",
    "\n",
    "# Names of classes\n",
    "names = [\"Knee Joint\"]\n",
    "\n",
    "# Relative path to validation images\n",
    "train = \"images/train\"\n",
    "\n",
    "# Relative path to validation images\n",
    "val = \"images/val\"\n",
    "\n",
    "# Number of classses\n",
    "nc = 1 \n",
    "\n",
    "createYaml(path, train, val, nc, names, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f8849",
   "metadata": {},
   "source": [
    "**Step 5: Fine-tuning YOLO Model**\n",
    "\n",
    "These steps fine-tune a YOLO11 nano segmentation model, along with a guide to understand the output metrics of the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce952fad",
   "metadata": {},
   "source": [
    "**Step 5.1/5.2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a YOLO model with the dataset. \n",
    "\n",
    "# Please make sure the .yaml file is directly under this directory!\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import trainModel    \n",
    "\n",
    "# The directory to your YOLO dataset\n",
    "dataset_dir = \"./YOLO Dataset\"\n",
    "\n",
    "# Path to your model. Put None if training brand new model\n",
    "model_path = None\n",
    "\n",
    "# Set number of epochs\n",
    "epoch_num = 250\n",
    "\n",
    "# Toggle simplified output\n",
    "simple_output = True\n",
    "\n",
    "# 1 simple output per number of epochs\n",
    "simple_output_per_epoch = 10\n",
    "\n",
    "\n",
    "trainModel(dataset_dir, model_path, epoch_num, simple_output, simple_output_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6add8",
   "metadata": {},
   "source": [
    "**Step 5.3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6596bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KneeDetectionLateral_TrainYoloModel import extraResultPlots\n",
    "\n",
    "json_file = \"./YOLO Dataset/output_data.json\"\n",
    "\n",
    "extraResultPlots(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaeb5e0",
   "metadata": {},
   "source": [
    "**Step 6: Inference with Tuned Model**\n",
    "\n",
    "These steps run an inference test with completely new images, testing the performance and capabilities of the trained model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898c241",
   "metadata": {},
   "source": [
    "**Step 6.1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8645de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Inference .json files\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import LMtoYOLO\n",
    "\n",
    "# Input target JSON files\n",
    "json_files = glob.glob(\"./Inference Images/**\")\n",
    "\n",
    "# Input output directory\n",
    "output_dir = \"./Inference Labels\"\n",
    "\n",
    "# Run conversion for all files\n",
    "for file in json_files:\n",
    "    if file.endswith(\".json\"):\n",
    "        LMtoYOLO(file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc14c8a",
   "metadata": {},
   "source": [
    "**Step 6.2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353faf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main inference program:\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import inferenceCheck, calculatePredictBox, calculateTruthBox, plotBoundingBox, manualDice, manualIoU, manual_mAP\n",
    "        \n",
    "# Load trained model \n",
    "model = YOLO(\"./runs/segment/train/weights/best.pt\")\n",
    "\n",
    "# Input target PNG files\n",
    "img_files = glob.glob(\"./Inference Images/**\")\n",
    "\n",
    "# Input labels directory\n",
    "txt_dir = \"./Inference Labels\"\n",
    "\n",
    "# Toggle displaying each prediction image in a new window\n",
    "window_per_img = False\n",
    "\n",
    "# Initialize matploblib figure (sizse: w,h in inches)\n",
    "figure = plt.figure(figsize=(32, 48))\n",
    "\n",
    "# Set the format of subplots. The number of rows should be a factor of the number of images\n",
    "rows = 1\n",
    "\n",
    "cols = int(len(results)/rows)\n",
    "figure.subplots(rows,cols)\n",
    "\n",
    "# Create lists for final processing\n",
    "IoU_list = []\n",
    "Dice_list = []\n",
    "\n",
    "# Run results through model\n",
    "results = inferenceCheck(model, img_files, False)\n",
    "\n",
    "# Run once per image, with incrementing index\n",
    "for index, result in enumerate(results):\n",
    "\n",
    "    # Swtich to correct subplot\n",
    "    subplot = plt.subplot(rows,cols,index+1)\n",
    "\n",
    "    # Display image in subplot\n",
    "    subplot.imshow(Image.open(result.path), cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "    # Retrieve information from predicted result\n",
    "    predict_box = calculatePredictBox(result)\n",
    "\n",
    "    # Plot predicted bounding box\n",
    "    plotBoundingBox(float(predict_box[\"x_min\"]), float(predict_box[\"y_min\"]), float(predict_box[\"box_width\"]), float(predict_box[\"box_height\"]), subplot, \"r\")\n",
    "\n",
    "    # Retrieve information from ground truth\n",
    "    truth_box = calculateTruthBox(result, txt_dir, predict_box[\"img_width\"], predict_box[\"img_height\"])\n",
    "    print(truth_box)\n",
    "\n",
    "    # Plot ground truth bounding box\n",
    "    plotBoundingBox(truth_box[\"x_min\"], truth_box[\"y_min\"], truth_box[\"box_width\"], truth_box[\"box_height\"], subplot, \"g\")\n",
    "\n",
    "    # Calculate intersection over union \n",
    "    IoU_score =  manualIoU(predict_box, truth_box)\n",
    "    IoU_list.append(IoU_score)\n",
    "    mAP50, mAP50_95 = manual_mAP(IoU_list)\n",
    "\n",
    "    # Calculate dice coefficient\n",
    "    Dice_score = manualDice(predict_box, truth_box)\n",
    "    Dice_list.append(Dice_score)\n",
    "\n",
    "\n",
    "    # Add labels to plot\n",
    "    plt.title(truth_box[\"file_name\"] + \".png\")\n",
    "    plt.xlabel(\"IoU Score: {}\\nDice Score: {}\".format(IoU_score, Dice_score))\n",
    "\n",
    "# Display plotted figure\n",
    "plt.show()\n",
    "print(\"Inference check complete!\")\n",
    "\n",
    "# Output statistics\n",
    "print(\"Number of Images: {}\\n\".format(len(results)))\n",
    "print(\"Highest IoU (Intersection over Union): {}\".format(max(IoU_list)))\n",
    "print(\"Lowest IoU (Intersection over Union): {}\".format(min(IoU_list)))\n",
    "print(\"Average IoU (Intersection over Union): {}\".format(sum(IoU_list)/len(IoU_list)))\n",
    "print(\"mAP50: {}\".format(mAP50))\n",
    "print(\"mAP50-95: {}\".format(mAP50_95))\n",
    "print(\"Highest Dice Coefficient: {}\".format(max(Dice_list)))\n",
    "print(\"Lowest Dice Coefficient: {}\".format(min(Dice_list)))\n",
    "print(\"Average Dice Coefficient: {}\".format(sum(Dice_list)/len(Dice_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60eca8",
   "metadata": {},
   "source": [
    "**Step 7: Cropping Images with Tuned Model**\n",
    "\n",
    "If inference yielded successful or expected results, the model can be used to crop images to their region of interest. These steps crop the image and process the image for the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f1923",
   "metadata": {},
   "source": [
    "**Step 7.1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KneeDetectionLateral_ImageProcessing import predict_img, crop_img\n",
    "\n",
    "from KneeDetectionLateral_TrainYoloModel import calculatePredictBox, calculateSquareBox\n",
    "\n",
    "# Input target PNG files\n",
    "model = YOLO(\"./runs/segment/train/weights/best.pt\")\n",
    "\n",
    "# Input target PNG files\n",
    "img_files = glob.glob(\"./Processing Images/**\")\n",
    "\n",
    "# Input output directory\n",
    "out_dir = \"./Processing Images Cropped\"\n",
    "\n",
    "# The part of the bounding box to be changed to create a square bounding box. Enter \"None\" to bypass.\n",
    "square_from = \"Bottom\"\n",
    "\n",
    "for file in img_files:\n",
    "    result = predict_img(model, file)\n",
    "\n",
    "    predict_box = calculatePredictBox(result)\n",
    "\n",
    "    adjust_box = calculateSquareBox(predict_box, square_from)\n",
    "\n",
    "    crop_img(file, out_dir, float(adjust_box[\"x_min\"]), float(adjust_box[\"y_min\"]), float(adjust_box[\"box_width\"]), float(adjust_box[\"box_height\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb91e6b",
   "metadata": {},
   "source": [
    "**Step 7.2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027024ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise output images\n",
    "\n",
    "from KneeDetectionLateral_ImageProcessing import normal_img\n",
    "\n",
    "img_files = glob.glob(\"./Processing Images Cropped/**\")\n",
    "output_dir = \"./Processing Images Normalised\"\n",
    "\n",
    "for file in img_files:\n",
    "    if file.endswith(\".png\"):\n",
    "      normal_img(file, True, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLAHE Normalisation on output images\n",
    "\n",
    "from KneeDetectionLateral_ImageProcessing import apply_clahe\n",
    "\n",
    "img_files = glob.glob(\"./Processing Images Normalised/**\")\n",
    "output_dir = \"./Processing Images CLAHE\"\n",
    "\n",
    "for file in img_files:\n",
    "    if file.endswith(\".png\"):\n",
    "      apply_clahe(file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823273b",
   "metadata": {},
   "source": [
    "**Achknowledgement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f568",
   "metadata": {},
   "source": [
    "@software{yolov8_ultralytics,\n",
    "  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n",
    "  title = {Ultralytics YOLOv8},\n",
    "  version = {8.0.0},\n",
    "  year = {2023},\n",
    "  url = {https://github.com/ultralytics/ultralytics},\n",
    "  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n",
    "  license = {AGPL-3.0}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
